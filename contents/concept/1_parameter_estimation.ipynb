{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Parameter Estimation\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Parameter estimation is the foundation of machine learning and deep learning. In this notebook, we'll learn how neural networks learn by working through a simple example: converting temperatures from an unknown scale to Celsius.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to define a parametric model\n",
    "- What a loss function is and why we need it\n",
    "- How gradient descent optimizes model parameters\n",
    "- Common pitfalls (exploding gradients) and their solutions\n",
    "- The importance of data normalization\n",
    "\n",
    "This hands-on example will give you intuition for the training process that powers all deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import our required libraries and configure PyTorch's output formatting for cleaner display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Temperature Conversion\n",
    "\n",
    "We have 11 temperature measurements in an unknown unit (`t_u`) and their corresponding values in Celsius (`t_c`). Our goal is to learn the relationship between these two scales.\n",
    "\n",
    "**The underlying relationship:** These measurements follow a linear relationship: `celsius = w Ã— unknown + b`\n",
    "\n",
    "Our task is to **estimate** the parameters `w` (weight/slope) and `b` (bias/intercept) from the data. This is exactly what machine learning does - it finds the best parameters that fit the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "A **model** is a function that takes inputs and parameters, and produces predictions. Here we use the simplest possible model: a linear function.\n",
    "\n",
    "```\n",
    "prediction = w Ã— input + b\n",
    "```\n",
    "\n",
    "This is the same as the equation of a line: `y = mx + b`\n",
    "\n",
    "- `w` (weight): Controls the slope - how much the output changes for each unit of input\n",
    "- `b` (bias): Controls the intercept - the output when input is zero\n",
    "\n",
    "Our model has only 2 parameters to learn, making it perfect for understanding the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "The **loss function** measures how wrong our predictions are. It's a single number that quantifies the difference between our model's predictions and the true values.\n",
    "\n",
    "Here we use **Mean Squared Error (MSE)**:\n",
    "1. Calculate the difference between prediction and truth: `(prediction - truth)`\n",
    "2. Square each difference to make all errors positive: `(prediction - truth)Â²`\n",
    "3. Take the average: `mean((prediction - truth)Â²)`\n",
    "\n",
    "**Why square the errors?**\n",
    "- Penalizes large errors more heavily (which is usually desirable)\n",
    "- Makes the math convenient for computing gradients\n",
    "- Always produces a positive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prediction\n",
    "\n",
    "Let's make our first prediction with naive initial parameters:\n",
    "- `w = 1.0` (assume a 1:1 relationship)\n",
    "- `b = 0.0` (assume no offset)\n",
    "\n",
    "With these initial values, our model simply returns the input unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
       "        21.8000, 48.4000, 60.4000, 68.4000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Loss\n",
    "\n",
    "The loss of `1763.88` tells us our initial parameters are very wrong! This large loss value will guide us in the right direction to improve our parameters.\n",
    "\n",
    "**Goal of training:** Adjust `w` and `b` to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Broadcasting\n",
    "\n",
    "Before we continue, let's understand an important PyTorch feature: **broadcasting**.\n",
    "\n",
    "When we operate on tensors of different shapes, PyTorch automatically expands them to compatible shapes. This allows our model to process multiple inputs at once (vectorization).\n",
    "\n",
    "**Key rules:**\n",
    "1. Scalar tensors can broadcast to any shape\n",
    "2. Dimensions of size 1 can expand to match other dimensions\n",
    "3. Operations work element-wise after broadcasting\n",
    "\n",
    "In our model, `w` and `b` are scalars that broadcast across all 11 temperature measurements simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: x: torch.Size([]), y: torch.Size([3, 1])\n",
      "        z: torch.Size([1, 3]), a: torch.Size([2, 1, 1])\n",
      "x * y: torch.Size([3, 1])\n",
      "y * z: torch.Size([3, 3])\n",
      "y * z * a: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(())\n",
    "y = torch.ones(3,1)\n",
    "z = torch.ones(1,3)\n",
    "a = torch.ones(2, 1, 1)\n",
    "print(f\"shapes: x: {x.shape}, y: {y.shape}\")\n",
    "print(f\"        z: {z.shape}, a: {a.shape}\")\n",
    "print(\"x * y:\", (x * y).shape)\n",
    "print(\"y * z:\", (y * z).shape)\n",
    "print(\"y * z * a:\", (y * z * a).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients: Numerical Approximation\n",
    "\n",
    "To improve our parameters, we need to know **which direction** to adjust them. This is determined by the **gradient** - the rate of change of the loss with respect to each parameter.\n",
    "\n",
    "**Numerical gradient approximation** uses the definition of derivative:\n",
    "```\n",
    "âˆ‚loss/âˆ‚w â‰ˆ [loss(w + Î´) - loss(w - Î´)] / (2Î´)\n",
    "```\n",
    "\n",
    "We slightly perturb the parameter (`w + Î´` and `w - Î´`), measure how the loss changes, and estimate the gradient.\n",
    "\n",
    "**Interpretation:** \n",
    "- If gradient is positive: loss increases when we increase `w` â†’ should decrease `w`\n",
    "- If gradient is negative: loss decreases when we increase `w` â†’ should increase `w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = \\\n",
    "    (loss_fn(model(t_u, w + delta, b), t_c) - \n",
    "     loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Step\n",
    "\n",
    "Now we update the parameter using **gradient descent**:\n",
    "\n",
    "```\n",
    "w_new = w_old - learning_rate Ã— gradient\n",
    "```\n",
    "\n",
    "**Why the negative sign?** We move in the **opposite direction** of the gradient to go downhill toward lower loss.\n",
    "\n",
    "**Learning rate** (`1e-2 = 0.01`): Controls how big of a step we take. Too large â†’ unstable, too small â†’ slow convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "    (loss_fn(model(t_u, w, b + delta), t_c) - \n",
    "     loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "\n",
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the bias parameter `b`. Both parameters need to be updated to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Gradients\n",
    "\n",
    "Numerical gradients work but are slow (require 2 forward passes per parameter). We can do better!\n",
    "\n",
    "Using **calculus**, we can derive exact gradient formulas. For MSE loss and linear model:\n",
    "\n",
    "**Loss gradient:**\n",
    "```\n",
    "âˆ‚loss/âˆ‚prediction = 2(prediction - truth) / n\n",
    "```\n",
    "\n",
    "This gives us the gradient of the loss with respect to each prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model gradient with respect to w:**\n",
    "\n",
    "Since `prediction = w Ã— input + b`, the derivative is:\n",
    "```\n",
    "âˆ‚prediction/âˆ‚w = input\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model gradient with respect to b:**\n",
    "\n",
    "```\n",
    "âˆ‚prediction/âˆ‚b = 1\n",
    "```\n",
    "\n",
    "The bias affects the prediction with a constant factor of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining gradients with the chain rule:**\n",
    "\n",
    "To get the gradient of loss with respect to parameters, we use the **chain rule**:\n",
    "\n",
    "```\n",
    "âˆ‚loss/âˆ‚w = âˆ‚loss/âˆ‚prediction Ã— âˆ‚prediction/âˆ‚w\n",
    "âˆ‚loss/âˆ‚b = âˆ‚loss/âˆ‚prediction Ã— âˆ‚prediction/âˆ‚b\n",
    "```\n",
    "\n",
    "We compute these for each data point, then sum them up to get the total gradient. This is the gradient we'll use to update our parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c) / t_p.size(0)  # <1>\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Now we put it all together! A **training loop** repeats these steps:\n",
    "\n",
    "1. **Forward pass**: Compute predictions with current parameters\n",
    "2. **Compute loss**: Measure how wrong the predictions are\n",
    "3. **Backward pass**: Compute gradients\n",
    "4. **Update parameters**: Take a step in the direction that reduces loss\n",
    "\n",
    "We repeat this for `n_epochs` iterations. Each epoch is one pass through the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss))) # <3>\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Training Loop\n",
    "\n",
    "This version adds:\n",
    "- **Selective printing**: Only print at interesting epochs to reduce clutter\n",
    "- **Gradient monitoring**: See how gradients change over time\n",
    "- **Stability check**: Stop if loss becomes infinite (training diverged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
    "                  print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            if print_params:\n",
    "                print('    Params:', params)\n",
    "                print('    Grad:  ', grad)\n",
    "        if epoch in {4, 12, 101}:\n",
    "            print('...')\n",
    "\n",
    "        if not torch.isfinite(loss).all():\n",
    "            break  # <3>\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "    Params: tensor([-44.1730,  -0.8260])\n",
      "    Grad:   tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 5802484.500000\n",
      "    Params: tensor([2568.4011,   45.1637])\n",
      "    Grad:   tensor([-261257.4062,   -4598.9702])\n",
      "Epoch 3, Loss 19408029696.000000\n",
      "    Params: tensor([-148527.7344,   -2616.3931])\n",
      "    Grad:   tensor([15109614.0000,   266155.6875])\n",
      "...\n",
      "Epoch 10, Loss 90901105189019073810297959556841472.000000\n",
      "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "    Grad:   tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 11, Loss inf\n",
      "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "    Grad:   tensor([1.8912e+21, 3.3313e+19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.8590e+19, -3.2746e+17])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Exploding Gradients\n",
    "\n",
    "ðŸš¨ **Watch what happens when we train with learning rate `1e-2`:**\n",
    "\n",
    "The loss **explodes** to infinity! Why?\n",
    "\n",
    "1. Initial gradients are very large (4517 for w!)\n",
    "2. Large learning rate Ã— large gradient = huge parameter update\n",
    "3. Parameters overshoot and move away from the optimum\n",
    "4. Loss increases, making gradients even larger\n",
    "5. Parameters grow exponentially â†’ **divergence**\n",
    "\n",
    "**Key lesson:** Learning rate must be chosen carefully. Too large â†’ unstable, too small â†’ slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "    Params: tensor([ 0.5483, -0.0083])\n",
      "    Grad:   tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 323.090515\n",
      "    Params: tensor([ 0.3623, -0.0118])\n",
      "    Grad:   tensor([1859.5493,   35.7843])\n",
      "Epoch 3, Loss 78.929634\n",
      "    Params: tensor([ 0.2858, -0.0135])\n",
      "    Grad:   tensor([765.4666,  16.5122])\n",
      "...\n",
      "Epoch 10, Loss 29.105247\n",
      "    Params: tensor([ 0.2324, -0.0166])\n",
      "    Grad:   tensor([1.4803, 3.0544])\n",
      "Epoch 11, Loss 29.104168\n",
      "    Params: tensor([ 0.2323, -0.0169])\n",
      "    Grad:   tensor([0.5781, 3.0384])\n",
      "...\n",
      "Epoch 99, Loss 29.023582\n",
      "    Params: tensor([ 0.2327, -0.0435])\n",
      "    Grad:   tensor([-0.0533,  3.0226])\n",
      "Epoch 100, Loss 29.022667\n",
      "    Params: tensor([ 0.2327, -0.0438])\n",
      "    Grad:   tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-4, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Smaller Learning Rate\n",
    "\n",
    "By reducing the learning rate to `1e-4` (100Ã— smaller), training becomes stable!\n",
    "\n",
    "**Observations:**\n",
    "- Loss steadily decreases from 1763 â†’ 29\n",
    "- Parameters converge to reasonable values\n",
    "- Gradients are still large, but smaller steps prevent divergence\n",
    "\n",
    "**However**, there's still a problem: why are the initial gradients so large (4517)? The root cause is the scale of our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "    Params: tensor([1.7761, 0.1064])\n",
      "    Grad:   tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574913\n",
      "    Params: tensor([2.0848, 0.1303])\n",
      "    Grad:   tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "    Params: tensor([2.2094, 0.1217])\n",
      "    Grad:   tensor([-12.4631,   0.8587])\n",
      "...\n",
      "Epoch 10, Loss 29.030489\n",
      "    Params: tensor([ 2.3232, -0.0710])\n",
      "    Grad:   tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941877\n",
      "    Params: tensor([ 2.3284, -0.1003])\n",
      "    Grad:   tensor([-0.5240,  2.9264])\n",
      "...\n",
      "Epoch 99, Loss 22.214186\n",
      "    Params: tensor([ 2.7508, -2.4910])\n",
      "    Grad:   tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148710\n",
      "    Params: tensor([ 2.7553, -2.5162])\n",
      "    Grad:   tensor([-0.4446,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_un, # <1>\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Solution: Data Normalization\n",
    "\n",
    "Instead of just reducing the learning rate, let's fix the root cause: **input scale**.\n",
    "\n",
    "Our input values (`t_u`) range from 21 to 82 - these are large numbers! When we multiply by `w` in our model, outputs become even larger, leading to large gradients.\n",
    "\n",
    "**Normalization**: Scale inputs to a smaller range by multiplying by `0.1`:\n",
    "- Original: [21.8, 33.9, 35.7, ..., 81.9]\n",
    "- Normalized: [2.18, 3.39, 3.57, ..., 8.19]\n",
    "\n",
    "This makes the optimization landscape smoother and gradients more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Normalized Data\n",
    "\n",
    "Now with normalized inputs, we can use the larger learning rate (`1e-2`) successfully!\n",
    "\n",
    "**Key improvements:**\n",
    "- Initial loss is much smaller (80 vs 1763)\n",
    "- Gradients are reasonable size (~77 vs 4517)\n",
    "- Training is stable\n",
    "- Loss decreases smoothly\n",
    "\n",
    "**Note:** Final parameters are different because we changed the input scale. The underlying relationship is still correctly learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Training for Better Convergence\n",
    "\n",
    "Let's train for 5000 epochs to see the full convergence behavior:\n",
    "\n",
    "**Observations:**\n",
    "- Loss continues to decrease: 80 â†’ 22 â†’ 2.93\n",
    "- Final parameters: `w â‰ˆ 5.37`, `b â‰ˆ -17.30`\n",
    "- Gradients become very small (0.0006, 0.0033) indicating convergence\n",
    "\n",
    "We've successfully found the parameters that best fit our data! \n",
    "\n",
    "**Fun fact:** If the unknown unit is Fahrenheit, the true conversion is `C = (F - 32) / 1.8` or `C = 0.556F - 17.78`, which is very close to our learned values when accounting for the 0.1 scaling factor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_un, \n",
    "    t_c = t_c,\n",
    "    print_params = True)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Results\n",
    "\n",
    "Let's see how well our trained model fits the data:\n",
    "\n",
    "- **Blue line**: Our model's predictions\n",
    "- **Orange dots**: Actual measurements\n",
    "\n",
    "The model captures the linear trend well! This visualization confirms that our learned parameters successfully model the temperature conversion relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_p = model(t_un, *params)  # <1>\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Temperature (Â°Fahrenheit)\")\n",
    "plt.ylabel(\"Temperature (Â°Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy()) # <2>\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.savefig(\"temp_unknown_plot.png\", format=\"png\")  # bookskip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Data Scatter Plot\n",
    "\n",
    "Here's a look at our raw data before fitting the model. The clear linear pattern suggests that a linear model is appropriate for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Measurement\")\n",
    "plt.ylabel(\"Temperature (Â°Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "\n",
    "plt.savefig(\"temp_data_plot.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "Congratulations! You've just learned the fundamentals of training neural networks:\n",
    "\n",
    "### Core Concepts\n",
    "1. **Model**: A parameterized function that makes predictions (`prediction = w Ã— input + b`)\n",
    "2. **Loss Function**: Measures prediction error (MSE in this case)\n",
    "3. **Gradients**: Direction and magnitude to adjust parameters\n",
    "4. **Gradient Descent**: Iterative algorithm to minimize loss\n",
    "\n",
    "### Practical Lessons\n",
    "5. **Learning Rate Matters**: Too large â†’ divergence, too small â†’ slow convergence\n",
    "6. **Normalization is Crucial**: Scale inputs to reasonable ranges for stable training\n",
    "7. **Monitor Training**: Watch loss and gradients to diagnose problems\n",
    "8. **Patience Pays Off**: More training epochs â†’ better convergence\n",
    "\n",
    "### What's Next?\n",
    "This same process scales to deep neural networks with millions of parameters! The concepts you've learned here - forward pass, loss computation, backward pass (gradients), and parameter updates - are the foundation of all deep learning.\n",
    "\n",
    "**Coming up**: We'll learn how PyTorch automates gradient computation with `autograd`, making it easy to train complex models without manually deriving gradient formulas!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
