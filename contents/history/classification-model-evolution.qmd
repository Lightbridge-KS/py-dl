---
title: "The Evolution of Classification Models: A Historical Tour"
---

Welcome! Let's journey through the fascinating history of classification models in deep learning. I'll guide you from the earliest biological inspirations to modern architectures.


## ğŸ§  Era 1: Biological Inspiration (1943)

**The Beginning: McCulloch-Pitts Neuron**

Everything started when Warren McCulloch (neuroscientist) and Walter Pitts (logician) proposed a mathematical model of a biological neuron.

```
    Biological Neuron                Mathematical Model
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
         Dendrites                        Inputs
            â”‚â”‚â”‚                            â”‚â”‚â”‚
            â–¼â–¼â–¼                            â–¼â–¼â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Cell   â”‚                    â”‚  Sum &  â”‚
       â”‚  Body   â”‚      â”€â”€â”€â”€â”€â”€â”€â–º      â”‚Thresholdâ”‚
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
            â”‚                              â”‚
         Axon                           Output
            â”‚                              â”‚
            â–¼                              â–¼
       To next neuron                  0 or 1
```

**Key Idea:** A neuron "fires" (outputs 1) if the sum of its inputs exceeds a threshold.


## âš¡ Era 2: The Perceptron (1958)

**Frank Rosenblatt's Breakthrough**

Rosenblatt added **weights** to inputs, making the model *learnable*.

```
                    THE PERCEPTRON
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
        Inputs         Weights         
          â”‚              â”‚             
          â–¼              â–¼             
                                       
         xâ‚ â”€â”€â”€â”€â–º( wâ‚ )â”€â”€â”€â”            
                          â”‚            
         xâ‚‚ â”€â”€â”€â”€â–º( wâ‚‚ )â”€â”€â”€â”¼â”€â”€â”€â–º[ Î£ ]â”€â”€â”€â–º[ step ]â”€â”€â”€â–º Å·
                          â”‚       â–²         â”‚
         xâ‚ƒ â”€â”€â”€â”€â–º( wâ‚ƒ )â”€â”€â”€â”˜       â”‚         â”‚
                                  â”‚         â–¼
                               bias (b)   0 or 1
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Mathematical Form:
    
        Å· = step(wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b)
        
                    â”Œ 1  if z â‰¥ 0
        step(z) =   â”‚
                    â”” 0  if z < 0
```

**Learning Rule (Perceptron Algorithm):**
```
For each training example:
    If prediction is wrong:
        w = w + Î± Â· (y - Å·) Â· x
        
    where Î± = learning rate
          y = true label
          Å· = predicted label
```

**What it could do:** Linear binary classification (e.g., AND, OR gates)


## ğŸ’€ Era 3: The First AI Winter (1969)

**The XOR Problem - Minsky & Papert's Critique**

Marvin Minsky and Seymour Papert published "Perceptrons" showing a fatal flaw:

```
         THE XOR PROBLEM
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
       xâ‚‚                             
        â”‚                             
      1 â”¤     â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹           
        â”‚     â”‚           â”‚           
        â”‚     â”‚           â”‚           
      0 â”¤     â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—           
        â”‚                             
        â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–º xâ‚  
              0     â”‚     1           
                    â”‚                 
                    
        â— = Class 1 (output 1)        
        â—‹ = Class 0 (output 0)        
                                      
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    XOR Truth Table:
    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ xâ‚ â”‚ xâ‚‚ â”‚ xâ‚ XOR xâ‚‚â”‚
    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  0 â”‚  0 â”‚    0     â”‚
    â”‚  0 â”‚  1 â”‚    1     â”‚
    â”‚  1 â”‚  0 â”‚    1     â”‚
    â”‚  1 â”‚  1 â”‚    0     â”‚
    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Problem: No single line can separate â— from â—‹ !
```

**The Crisis:** A single perceptron can only learn *linearly separable* patterns. XOR is NOT linearly separable.

â†’ This triggered the **First AI Winter** (funding dried up for ~15 years)


## ğŸ”¥ Era 4: Multi-Layer Perceptron & Backpropagation (1986)

**The Renaissance: Rumelhart, Hinton & Williams**

The solution to XOR? **Add more layers!**

```
      MULTI-LAYER PERCEPTRON (MLP)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
       Input        Hidden Layer        Output
       Layer        (the magic!)        Layer
    
                      â”Œâ”€â”€â”€â”
         xâ‚ â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ hâ‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â•²      â””â”€â”€â”€â”˜      â•±  â”‚
                â•²              â•±    â”‚    â”Œâ”€â”€â”€â”
                 â•²           â•±      â””â”€â”€â”€â”€â”¤   â”‚
                  â•²        â•±       â”Œâ”€â”€â”€â”€â”€â”¤ Å· â”œâ”€â”€â”€â–º Output
                   â•²     â•±         â”‚     â”‚   â”‚
         xâ‚‚ â—‹â”€â”€â”€â”€â”€â”€â”€â•²â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”˜
               â•²     â•²â•±      â•±
                â•²    â•±â•²    â•±
                 â•² â•±   â•² â•±
                 â•±â•²    â•± â•²
               â•±    â•²â•±    â•²
              â•±     â•±â•²     â•²      â”Œâ”€â”€â”€â”
         bâ‚ â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ hâ‚‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€
                                  â””â”€â”€â”€â”˜
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Key Innovation: NON-LINEAR ACTIVATION FUNCTIONS
    
    Instead of step function, use smooth functions:
    
    Sigmoid: Ïƒ(z) = 1 / (1 + e^(-z))
    
              1 â”¤        ___________
                â”‚      â•±
                â”‚     â”‚
              0.5     â”‚
                â”‚     â”‚
                â”‚____â•±
              0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º z
                    -4   0   4
```

**The Backpropagation Algorithm:**

```
    FORWARD PASS                      BACKWARD PASS
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Input â”€â”€â”€â”€â”€â”€â–º                     â—„â”€â”€â”€â”€â”€â”€ âˆ‚Loss/âˆ‚w
                                              (gradients)
         Layer 1 â”€â”€â”€â”€â”€â”€â–º              â—„â”€â”€â”€â”€â”€â”€
                                              Chain Rule!
         Layer 2 â”€â”€â”€â”€â”€â”€â–º              â—„â”€â”€â”€â”€â”€â”€
                      
         Output â”€â”€â”€â”¬â”€â”€â”€â–º              â—„â”€â”€â”€â”€â”€â”€
                   â”‚                      â–²
                   â–¼                      â”‚
               Prediction â”€â”€â”€â”€â”€â–º Loss â”€â”€â”€â”€â”˜
                   vs
                 Target
    
    
    Update Rule (Gradient Descent):
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
        w_new = w_old - Î± Â· âˆ‚Loss/âˆ‚w
        
    The chain rule allows us to compute gradients
    for ALL layers efficiently!
```


## ğŸ–¼ï¸ Era 5: Convolutional Neural Networks (1989-1998)

**LeCun's LeNet - Designed for Images**

Yann LeCun realized: for images, we need **spatial awareness**.

```
    WHY CONVOLUTION? - The Problem with MLPs for Images
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    A 28Ã—28 image = 784 pixels
    
    MLP approach: Flatten â†’ 784 inputs â†’ Fully connected
    
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Image      â”‚        â”‚ â€¢ â€¢ â€¢ â€¢ â€¢   â”‚
         â”‚  28 Ã— 28    â”‚  â”€â”€â”€â–º  â”‚ 784 inputs  â”‚  â”€â”€â”€â–º Many weights!
         â”‚             â”‚        â”‚ â€¢ â€¢ â€¢ â€¢ â€¢   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Problems:
    1. TOO MANY parameters (784 Ã— hidden_size)
    2. NO spatial structure preserved
    3. NOT translation invariant
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**The CNN Solution:**

```
    CONVOLUTIONAL NEURAL NETWORK (LeNet-5, 1998)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Input      Conv      Pool      Conv      Pool      FC      Output
    32Ã—32      28Ã—28     14Ã—14     10Ã—10     5Ã—5       
    
    â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”
    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚â–“â–“â–“â”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚â–“â–“â–“â”‚   â”‚ F â”‚   â”‚ 0 â”‚
    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â”€â”€â–ºâ”‚â–‘â–‘â–‘â–‘â–‘â”‚â”€â”€â–ºâ”‚â–“â–“â–“â”‚â”€â”€â–ºâ”‚â–‘â–‘â–‘â–‘â–‘â”‚â”€â”€â–ºâ”‚â–“â–“â–“â”‚â”€â”€â–ºâ”‚ C â”‚â”€â”€â–ºâ”‚...â”‚
    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚â–“â–“â–“â”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚â–“â–“â–“â”‚   â”‚   â”‚   â”‚ 9 â”‚
    â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜
                â”‚                                          
                â–¼                                          
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    
    CONVOLUTION OPERATION:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Input (5Ã—5)              Kernel (3Ã—3)         Output
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       
    â”‚ 1  2  3  4  5   â”‚      â”‚ 1  0  -1  â”‚       Slide kernel
    â”‚ 6  7  8  9  10  â”‚  âœ±   â”‚ 1  0  -1  â”‚  â”€â”€â”€â–º across input
    â”‚11 12 13 14 15   â”‚      â”‚ 1  0  -1  â”‚       (detect edges!)
    â”‚16 17 18 19 20   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚21 22 23 24 25   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    POOLING (Downsampling):
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”
    â”‚ 1 3 â”‚ 2 4 â”‚           â”‚     â”‚
    â”‚ 5 7 â”‚ 6 8 â”‚   Max     â”‚ 7 8 â”‚    Reduce size
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤  â”€â”€â”€â”€â”€â–º   â”‚     â”‚    Keep important
    â”‚ 9 2 â”‚ 3 1 â”‚  Pooling  â”‚ 9 6 â”‚    features
    â”‚ 4 6 â”‚ 5 2 â”‚           â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”˜
```

**Key Benefits:**
- **Parameter sharing:** Same kernel used across entire image
- **Translation invariance:** Detect features anywhere in image
- **Hierarchical features:** Low-level â†’ High-level

This was **HUGE for radiology AI** â€” the same principles power medical image analysis today!


## ğŸŒ‹ Era 6: The Deep Learning Revolution (2006-2012)

**Geoffrey Hinton's Deep Belief Networks (2006)**

Hinton showed we could train *deep* networks using layer-by-layer pre-training.

**But the real explosion came in 2012...**

```
    ALEXNET (2012) - ImageNet Breakthrough
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ImageNet Challenge: Classify 1.2M images into 1000 categories
    
    
    Previous Best Error Rate:  ~26%   (traditional ML)
                                 â”‚
                                 â”‚  â† HUGE GAP!
                                 â”‚
    AlexNet Error Rate:        ~16%   (deep learning)
    
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    
    AlexNet Architecture:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Input: 224Ã—224Ã—3 (RGB image)
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Conv Layer 1  â”‚  96 filters, 11Ã—11
    â”‚   + ReLU + Pool â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Conv Layer 2  â”‚  256 filters, 5Ã—5
    â”‚   + ReLU + Pool â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Conv Layer 3  â”‚  384 filters, 3Ã—3
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Conv Layer 4  â”‚  384 filters, 3Ã—3
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Conv Layer 5  â”‚  256 filters, 3Ã—3
    â”‚   + Pool        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   FC Layer 1    â”‚  4096 neurons + Dropout
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   FC Layer 2    â”‚  4096 neurons + Dropout
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Output        â”‚  1000 classes (Softmax)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Total Parameters: ~60 Million!
```

**Key Innovations in AlexNet:**

```
    1. ReLU ACTIVATION (instead of Sigmoid)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Sigmoid Problem:           ReLU Solution:
    
      1â”‚    ___________         â”‚         â•±
       â”‚  â•±                     â”‚       â•±
       â”‚ â”‚                      â”‚     â•±
    0.5â”‚ â”‚                      â”‚   â•±
       â”‚ â”‚  â† Gradients         â”‚ â•±
       â”‚_â•±    vanish!           â”‚â•±____________
      0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Saturates!              ReLU(x) = max(0, x)
                                Never saturates for x > 0!
    
    
    2. DROPOUT (Regularization)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Training:                   Inference:
    
    â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹                   â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹
     â•² â•± â•² â•±                     â”‚â•²â”‚â•±â”‚â•²â”‚â•±â”‚
    â—‹â”€â•³â”€â—â”€â•³â”€â—‹   Randomly        â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹   Use all
     â•± â•² â•± â•²    drop 50%!        â”‚â•±â”‚â•²â”‚â•±â”‚â•²â”‚  neurons
    â—â”€â”€â”€â—‹â”€â”€â”€â—                   â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹
    
    â— = dropped (zeroed)        Prevents overfitting!
    
    
    3. GPU TRAINING
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    CPU: Sequential processing     GPU: Parallel processing
    
    [â– ]â”€â”€â–º[â– ]â”€â”€â–º[â– ]â”€â”€â–º[â– ]         [â– ][â– ][â– ][â– ][â– ][â– ][â– ][â– ]
                                  [â– ][â– ][â– ][â– ][â– ][â– ][â– ][â– ]
    Slow for matrix ops!          [â– ][â– ][â– ][â– ][â– ][â– ][â– ][â– ]
                                  
                                  MASSIVE parallelism!
```


## ğŸ—ï¸ Era 7: Going Deeper (2014-2016)

**The Depth Race:**

```
    NETWORK DEPTH EVOLUTION
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Year    Network          Layers      ImageNet Error
    â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    2012    AlexNet            8            16.4%
    2014    VGGNet            19            7.3%
    2014    GoogLeNet         22            6.7%
    2015    ResNet           152            3.6%  â† Superhuman!
    
    Human performance: ~5.1%
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**The Vanishing Gradient Problem Returns:**

```
    Deep Networks (before ResNet):
    
    Input â”€â”€â–º L1 â”€â”€â–º L2 â”€â”€â–º ... â”€â”€â–º L100 â”€â”€â–º Output
                                      â”‚
                                      â–¼
                                    Loss
                                      â”‚
    Backprop: gradients get smaller â—„â”€â”˜
              as they flow back
    
              âˆ‚Loss     âˆ‚Loss    âˆ‚L99   âˆ‚L98         âˆ‚L2
              â”€â”€â”€â”€â”€ =   â”€â”€â”€â”€â”€ Ã— â”€â”€â”€â”€â”€ Ã— â”€â”€â”€â”€â”€ Ã— ... Ã— â”€â”€â”€â”€â”€
              âˆ‚L1       âˆ‚L100   âˆ‚L100   âˆ‚L99          âˆ‚L1
              
              â”‚         â”‚        â”‚       â”‚            â”‚
              â–¼         â–¼        â–¼       â–¼            â–¼
              Tiny!    <1      <1      <1     ...    <1
              
    Early layers learn VERY slowly!
```

**ResNet's Brilliant Solution: Skip Connections**

```
    RESIDUAL BLOCK (ResNet, 2015)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Traditional:                    Residual:
    
         x                               x
         â”‚                               â”‚
         â–¼                               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚  Conv   â”‚                    â”‚  Conv   â”‚         â”‚ Identity
    â”‚  BN     â”‚                    â”‚  BN     â”‚         â”‚ Shortcut
    â”‚  ReLU   â”‚                    â”‚  ReLU   â”‚         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
    â”‚  Conv   â”‚                    â”‚  Conv   â”‚         â”‚
    â”‚  BN     â”‚                    â”‚  BN     â”‚         â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚
         â”‚                              â”‚              â”‚
         â–¼                              â–¼              â”‚
         H(x)                      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”‚
                                   â”‚    +    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                                    H(x) = F(x) + x
                                        â”‚
                                        â–¼
                                      ReLU
                                        â”‚
                                        â–¼
                                     Output
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    WHY IT WORKS:
    
    Instead of learning:  H(x)         (hard)
    Learn the residual:   F(x) = H(x) - x   (easier!)
    
    If identity mapping is optimal, just learn F(x) = 0
    
    Gradients can flow directly through skip connections!
    
         âˆ‚Loss    âˆ‚Loss     âˆ‚F(x)
         â”€â”€â”€â”€â”€ =  â”€â”€â”€â”€â”€ Ã— ( â”€â”€â”€â”€â”€ + 1 )
          âˆ‚x      âˆ‚H(x)      âˆ‚x      â–²
                                     â”‚
                          Always at least 1!
                          Gradient never vanishes!
```


## ğŸ¤– Era 8: The Attention Revolution (2017-Present)

**Transformers: "Attention Is All You Need"**

Originally designed for NLP, Transformers revolutionized everything, including vision.

```
    SELF-ATTENTION MECHANISM
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Problem: CNNs have LIMITED receptive field
    
    CNN:                              Attention:
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ±      ğŸ•          â”‚          â”‚ ğŸ± â†â”€â”€â”€â”€â”€ ğŸ•         â”‚
    â”‚    â”Œâ”€â”€â”€â”            â”‚          â”‚  â”‚ â•²     â•± â”‚        â”‚
    â”‚    â”‚3Ã—3â”‚ only sees  â”‚          â”‚  â”‚  â•²   â•±  â”‚        â”‚
    â”‚    â”‚ â–‘ â”‚ local area â”‚          â”‚  â”‚   â•² â•±   â”‚        â”‚
    â”‚    â””â”€â”€â”€â”˜            â”‚          â”‚  â–¼    â•³    â–¼        â”‚
    â”‚         ğŸŒ³          â”‚          â”‚  ğŸŒ³ â†â”€â•±â”€â•²â”€â†’ ğŸ        â”‚
    â”‚              ğŸ      â”‚          â”‚                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     
    Local context only!              GLOBAL context!
                                     Every position attends
                                     to every other position
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    
    ATTENTION FORMULA:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
                         Q Â· K^T
    Attention(Q,K,V) = softmax(â”€â”€â”€â”€â”€â”€â”€â”€â”€) Â· V
                               âˆšd_k
    
    Where:
    Q = Query  ("What am I looking for?")
    K = Key    ("What do I contain?")
    V = Value  ("What information do I provide?")
    
    
    Intuition with a sentence:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    "The cat sat on the mat because it was tired"
    
    What does "it" refer to?
    
    "it" (Query) attends to all words (Keys):
    
         The   cat   sat   on   the   mat   because   it   was   tired
          â”‚     â”‚     â”‚    â”‚     â”‚     â”‚       â”‚      â”‚     â”‚      â”‚
          â–¼     â–¼     â–¼    â–¼     â–¼     â–¼       â–¼      â–¼     â–¼      â–¼
    Attention:
         0.05  0.70  0.02 0.01  0.03  0.15   0.01   0.01  0.01   0.01
               â–²                       â–²
               â”‚                       â”‚
           Most attention!         Some attention
           
    "it" = "cat" (not "mat")
```

**Vision Transformer (ViT, 2020)**

```
    VISION TRANSFORMER
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Step 1: Split image into patches (like "words")
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚                 â”‚         â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚
    â”‚     Image       â”‚  â”€â”€â”€â–º   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
    â”‚    224Ã—224      â”‚         â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚
    â”‚                 â”‚         â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ 9 â”‚10 â”‚11 â”‚12 â”‚
                                â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
                                â”‚13 â”‚14 â”‚15 â”‚16 â”‚
                                â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
                                16 patches of 56Ã—56
    
    
    Step 2: Flatten & embed patches (+ position encoding)
    
    Patch 1  Patch 2  Patch 3  ...  Patch 16   [CLS]
       â”‚        â”‚        â”‚            â”‚          â”‚
       â–¼        â–¼        â–¼            â–¼          â–¼
    â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”
    â”‚Embedâ”‚  â”‚Embedâ”‚  â”‚Embedâ”‚ ... â”‚Embedâ”‚    â”‚Classâ”‚
    â”‚+Pos â”‚  â”‚+Pos â”‚  â”‚+Pos â”‚     â”‚+Pos â”‚    â”‚Tokenâ”‚
    â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜
       â”‚        â”‚        â”‚            â”‚          â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                       â”‚
                    â”‚  Transformer Encoder  â”‚  Ã— N layers
                    â”‚  (Self-Attention +    â”‚
                    â”‚   Feed-Forward)       â”‚
                    â”‚                       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                         [CLS] output
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Classification     â”‚
                    â”‚        Head           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                         Class prediction
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```


## ğŸ“Š Summary Timeline

```
    THE EVOLUTION OF CLASSIFICATION MODELS
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    1943 â”€â”€â—â”€â”€ McCulloch-Pitts Neuron (biological inspiration)
           â”‚
    1958 â”€â”€â—â”€â”€ Perceptron (first learnable classifier)
           â”‚
    1969 â”€â”€â—â”€â”€ XOR Problem / AI Winter (showed limitations)
           â”‚
           â”‚   ~15 years of "winter"
           â”‚
    1986 â”€â”€â—â”€â”€ Backpropagation (multi-layer networks work!)
           â”‚
    1989 â”€â”€â—â”€â”€ LeNet (CNNs for images)
           â”‚
    1998 â”€â”€â—â”€â”€ LeNet-5 (digit recognition deployed!)
           â”‚
    2006 â”€â”€â—â”€â”€ Deep Belief Networks (Hinton)
           â”‚
    2012 â”€â”€â—â”€â”€ AlexNet (ImageNet breakthrough!) â† DEEP LEARNING ERA BEGINS
           â”‚
    2014 â”€â”€â—â”€â”€ VGGNet, GoogLeNet (going deeper)
           â”‚
    2015 â”€â”€â—â”€â”€ ResNet (skip connections, 152 layers!)
           â”‚
    2017 â”€â”€â—â”€â”€ Transformers (attention mechanism)
           â”‚
    2020 â”€â”€â—â”€â”€ Vision Transformer (ViT)
           â”‚
    2023+ â”€â—â”€â”€ Foundation Models (GPT-4V, CLIP, etc.)
           â”‚
           â–¼
        Present: Multimodal models, medical AI...
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```


## ğŸ¥ Relevance to Radiology AI

As a radiologist, here's what's most relevant:

| Era | Model | Medical Imaging Application |
|-----|-------|----------------------------|
| 1998 | LeNet | Early CAD systems |
| 2012+ | AlexNet/VGG | Chest X-ray classification |
| 2015+ | ResNet/DenseNet | Current standard for medical imaging |
| 2020+ | ViT/Swin | Emerging in pathology & radiology |

**Most radiology AI today uses ResNet-like architectures** because:

1. Proven reliability
2. Good balance of depth vs. complexity
3. Pre-trained weights available
4. Works well with limited medical data