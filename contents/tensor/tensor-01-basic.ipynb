{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a594a09",
   "metadata": {},
   "source": [
    "# Tensor (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d74dfe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc6133",
   "metadata": {},
   "source": [
    "```\n",
    "TENSOR DIMENSIONS (Visual Guide):\n",
    "\n",
    "0D (Scalar):  [5.2]\n",
    "             (just a number)\n",
    "\n",
    "1D (Vector):  [1.2, 3.4, 5.6, 7.8]\n",
    "             (like pixel intensities along a line)\n",
    "\n",
    "2D (Matrix):  [[120, 150, 180],     ← Row 0\n",
    "               [90,  110, 140],     ← Row 1  \n",
    "               [60,  80,  100]]     ← Row 2\n",
    "              ↑     ↑     ↑\n",
    "           Col 0  Col 1  Col 2\n",
    "           (like a grayscale image)\n",
    "\n",
    "3D Tensor:    Multiple 2D slices stacked\n",
    "              (like CT scan slices)\n",
    "              \n",
    "4D Tensor:    [Batch, Channels, Height, Width]\n",
    "              (multiple images for training)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbf1ca",
   "metadata": {},
   "source": [
    "## Create Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7375db",
   "metadata": {},
   "source": [
    "### From Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0771e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee1c6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[120., 150., 180.],\n",
       "        [ 90., 110., 140.],\n",
       "        [ 60.,  80., 100.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values = [[120, 150, 180], [90, 110, 140], [60, 80, 100]]\n",
    "image_tensor = torch.tensor(pixel_values, dtype=torch.float32)\n",
    "image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cf578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de1ddcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9499d9",
   "metadata": {},
   "source": [
    "### From NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05197a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 64, 175,  82, ...,  15,  81, 218],\n",
       "       [251, 226, 178, ..., 219, 136, 232],\n",
       "       [238, 226,  33, ..., 243,  55,  14],\n",
       "       ...,\n",
       "       [183,  77, 146, ..., 208,  33, 132],\n",
       "       [208,  36, 247, ...,   3, 136,  44],\n",
       "       [ 30, 202, 102, ..., 158,  51, 181]], shape=(64, 64), dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_image = np.random.randint(0, 255, size=(64, 64), dtype=np.uint8)\n",
    "np_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b368777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 64., 175.,  82.,  ...,  15.,  81., 218.],\n",
       "        [251., 226., 178.,  ..., 219., 136., 232.],\n",
       "        [238., 226.,  33.,  ..., 243.,  55.,  14.],\n",
       "        ...,\n",
       "        [183.,  77., 146.,  ..., 208.,  33., 132.],\n",
       "        [208.,  36., 247.,  ...,   3., 136.,  44.],\n",
       "        [ 30., 202., 102.,  ..., 158.,  51., 181.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_image = torch.from_numpy(np_image)\n",
    "medical_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afaa7e8",
   "metadata": {},
   "source": [
    "### From Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9bdef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b916c257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retains the properties of x_data\n",
    "x_ones = torch.ones_like(x_data)\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "885cceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5339, 0.4105],\n",
       "        [0.1034, 0.5003]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overrides the datatype of x_data\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) \n",
    "x_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2748b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in tensor creation functions\n",
    "zeros_like_scan = torch.zeros(512, 512)  # Like an empty 512x512 image\n",
    "ones_mask = torch.ones(256, 256)         # Like a binary mask\n",
    "random_noise = torch.randn(100, 100)     # Gaussian noise (useful for testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390ffac",
   "metadata": {},
   "source": [
    "## Attributes of a Tensor\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e0d1bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb9f2",
   "metadata": {},
   "source": [
    "## Operations on Tensors\n",
    "\n",
    "Each of these operations can be run on the CPU and Accelerator such as CUDA, MPS, MTIA, or XPU. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2a2f8",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14ea6ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.accelerator.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cebb05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.accelerator.current_accelerator() # MacOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d468708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the current accelerator if available\n",
    "if torch.accelerator.is_available():\n",
    "    tensor = tensor.to(torch.accelerator.current_accelerator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82315a58",
   "metadata": {},
   "source": [
    "## Radiology Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25c36d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor representing a small CT volume (depth, height, width)\n",
    "ct_volume = torch.randn(32, 128, 128)  # 32 slices, 128x128 pixels each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1b8a7",
   "metadata": {},
   "source": [
    "### Index and Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdc83579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT Volume shape: torch.Size([32, 128, 128])\n",
      "Number of dimensions: 3\n",
      "Total number of elements: 524288\n",
      "Data type: torch.float32\n",
      "Device (CPU/GPU): cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"CT Volume shape: {ct_volume.shape}\")\n",
    "print(f\"Number of dimensions: {ct_volume.ndim}\")\n",
    "print(f\"Total number of elements: {ct_volume.numel()}\")\n",
    "print(f\"Data type: {ct_volume.dtype}\")\n",
    "print(f\"Device (CPU/GPU): {ct_volume.device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "636cd924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Example Index & Slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85dd20f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First slice shape: torch.Size([128, 128])\n",
      "ROI patch shape: torch.Size([5, 50, 50])\n",
      "Center pixel value: 0.562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indexing and slicing (like selecting ROI in medical images)\n",
    "first_slice = ct_volume[0]           # Get first slice\n",
    "roi_patch = ct_volume[10:15, 50:100, 50:100]  # 5 slices, specific region\n",
    "center_pixel = ct_volume[16, 64, 64]  # Single voxel value\n",
    "\n",
    "print(f\"First slice shape: {first_slice.shape}\")\n",
    "print(f\"ROI patch shape: {roi_patch.shape}\")\n",
    "print(f\"Center pixel value: {center_pixel.item():.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab275fcb",
   "metadata": {},
   "source": [
    "### Reshaping tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6682052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: torch.Size([32, 128, 128])\n",
      "Flattened: torch.Size([524288])\n",
      "Reshaped: torch.Size([32, 16384])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_shape = ct_volume.shape\n",
    "flattened = ct_volume.view(-1)  # Flatten to 1D (like vectorizing an image)\n",
    "reshaped = ct_volume.view(32, -1)  # Keep first dim, flatten others\n",
    "\n",
    "print(f\"Original: {original_shape}\")\n",
    "print(f\"Flattened: {flattened.shape}\")\n",
    "print(f\"Reshaped: {reshaped.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ded39",
   "metadata": {},
   "source": [
    "### Element-wise Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81b32281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image stats - mean: 0.004, std: 0.996\n",
      "Normalized image stats - mean: -0.000, std: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_a = torch.randn(256, 256)\n",
    "image_b = torch.randn(256, 256)\n",
    "\n",
    "# Element-wise operations (applied to each pixel)\n",
    "sum_images = image_a + image_b       # Add two images\n",
    "scaled_image = image_a * 0.5         # Scale intensity\n",
    "normalized = (image_a - image_a.mean()) / image_a.std()  # Z-score normalization\n",
    "\n",
    "print(f\"Original image stats - mean: {image_a.mean():.3f}, std: {image_a.std():.3f}\")\n",
    "print(f\"Normalized image stats - mean: {normalized.mean():.3f}, std: {normalized.std():.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250f188",
   "metadata": {},
   "source": [
    "### Arithmatic Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f450b",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11a28d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d234437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y1 == y2) & (y2 == y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb979065",
   "metadata": {},
   "source": [
    "#### Element-wise Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0f9ae35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321015b8",
   "metadata": {},
   "source": [
    "#### Single Element Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c021cd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "663c9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# you can convert it to a Python numerical value using item():\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10134a28",
   "metadata": {},
   "source": [
    "### In-place operations\n",
    "\n",
    "Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56d6e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9adc6a",
   "metadata": {},
   "source": [
    "## Other Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce17bd",
   "metadata": {},
   "source": [
    "### Transpose operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "807dd1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image: torch.Size([480, 640])\n",
      "Transposed: torch.Size([640, 480])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_2d = torch.randn(480, 640)  # Height x Width\n",
    "transposed = image_2d.t()         # Width x Height (transpose)\n",
    "print(f\"Original image: {image_2d.shape}\")\n",
    "print(f\"Transposed: {transposed.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da890f76",
   "metadata": {},
   "source": [
    "### Permute Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute for more complex dimension reordering\n",
    "batch_images = torch.randn(10, 3, 224, 224)  # Batch, Channels, Height, Width\n",
    "# Change to Batch, Height, Width, Channels (for some visualization libraries)\n",
    "reordered = batch_images.permute(0, 2, 3, 1)\n",
    "print(f\"Original batch: {batch_images.shape}\")\n",
    "print(f\"Reordered: {reordered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e7481",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a39a0e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 640])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "555f4b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended image (side by side): torch.Size([480, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Cat for concatenating along existing dimension\n",
    "extended_image = torch.cat([image_2d, image_2d], dim=1)  # Side by side\n",
    "print(f\"Extended image (side by side): {extended_image.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01a4dc",
   "metadata": {},
   "source": [
    "### Create Stack Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "616545a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation (joining tensors)\n",
    "slice_1 = torch.randn(128, 128)\n",
    "slice_2 = torch.randn(128, 128) \n",
    "slice_3 = torch.randn(128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcbf78b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual slice: torch.Size([128, 128])\n",
      "Stacked volume: torch.Size([3, 128, 128])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stack to create a volume\n",
    "volume = torch.stack([slice_1, slice_2, slice_3], dim=0)\n",
    "print(f\"Individual slice: {slice_1.shape}\")\n",
    "print(f\"Stacked volume: {volume.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0fa02d",
   "metadata": {},
   "source": [
    "## Image Processing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b28ea259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate loading a batch of medical images for training\n",
    "batch_size = 4\n",
    "num_slices = 16\n",
    "height, width = 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a0385e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical batch shape: torch.Size([4, 16, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create a batch of 3D medical volumes\n",
    "medical_batch = torch.randn(batch_size, num_slices, height, width)\n",
    "print(f\"Medical batch shape: {medical_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326a2a9",
   "metadata": {},
   "source": [
    "### Normalize each volume independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6137847d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 256])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da8fe842",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_batch = torch.zeros_like(medical_batch)\n",
    "for i in range(batch_size):\n",
    "    volume = medical_batch[i]\n",
    "    normalized_batch[i] = (volume - volume.mean()) / (volume.std() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8053ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 256, 256])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04f148",
   "metadata": {},
   "source": [
    "### Extract middle slices \n",
    "\n",
    "(common in 2D analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "middle_slices = medical_batch[:, num_slices//2, :, :]  # Shape: (batch_size, height, width)\n",
    "print(f\"Middle slices shape: {middle_slices.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e908941",
   "metadata": {},
   "source": [
    "### Create a simple feature map (like edge detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This simulates what a convolutional layer might do\n",
    "sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
