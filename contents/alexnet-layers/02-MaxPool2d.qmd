# MaxPool2d


::: {.callout-note appearance="default" collapse="false"}
**Q:** Now let's move on explaining MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False). Explains each of the parameters and draw plain text diagrams to illustrate it.
:::


```python
MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
```

This is the **first pooling layer** in AlexNet (after Conv+ReLU).

---

###  Parameters

1. **`kernel_size=3`**

   * The pooling window is **3×3**.
   * Looks at a small patch of the feature map at a time.

2. **`stride=2`**

   * The window moves **2 pixels** horizontally and vertically.
   * Reduces spatial size (downsampling).

3. **`padding=0`**

   * No extra border added.
   * Only covers valid inside regions.

4. **`dilation=1`**

   * Spacing between elements inside the pooling window.
   * With `1`, it’s standard pooling (no holes).
   * Larger dilation = spread out window → rarely used in pooling.

5. **`ceil_mode=False`**

   * Output size is computed with **floor** division.
   * If `True`, it would use ceiling instead (allowing one extra pooled element if partially covered).
   * With `False`, only fully covered windows are used.

---

###  Formula for output size

For input size $W$:

$$
\text{Output} = \left\lfloor \frac{W - K}{S} \right\rfloor + 1
$$

Where $K$=kernel, $S$=stride, $W$=input width/height.

So if input was `55×55` (from the first conv layer), then:

$$
\frac{55 - 3}{2} + 1 = 27
$$

Output = `27×27`.

---

###  What pooling does

Pooling just **takes the maximum value** in each sliding window.
It keeps the most **dominant activation** → encourages translational invariance.

---

###  Plain Text Diagrams

#### Input feature map slice (one channel, small toy 7×7 example)

```
Input (7x7):
 1 2 5 3 1 0 2
 0 4 6 2 9 1 3
 7 2 8 6 4 0 1
 3 1 9 5 2 8 4
 6 2 0 1 7 3 5
 4 8 3 2 6 1 0
 2 1 5 9 3 2 7
```

---

#### Step 1 — place 3×3 window (top-left)

```
[1 2 5] 3 1 0 2
[0 4 6] 2 9 1 3
[7 2 8] 6 4 0 1
```

* Max = **8** → goes to output(0,0)

---

#### Step 2 — move right by stride=2

```
1 2 [5 3 1] 0 2
0 4 [6 2 9] 1 3
7 2 [8 6 4] 0 1
```

* Max = **9** → output(0,1)

---

#### Step 3 — move down by stride=2

```
7 2 8 6 4 0 1
3 1 9 5 2 8 4
6 2 0 1 7 3 5
```

* Max = **9** → output(1,0)

---

#### Final Output (with stride=2, kernel=3)

Input 7×7 → Output 3×3:

```
[ 8  9  9
  9  9  8
  8  9  7 ]
```

---

**In AlexNet case**

* Input feature map: **55×55**
* Pooling: **3×3, stride=2**
* Output: **27×27** per channel
* Since we had 64 channels → **64×27×27** after this pooling.

---

 So this `MaxPool2d` layer:

* Downsamples spatial resolution (55 → 27)
* Keeps **only the strongest activations** per window
* Reduces computation and adds robustness to small translations.

---
