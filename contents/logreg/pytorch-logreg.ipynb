{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression with PyTorch\n",
    "\n",
    "## Overview\n",
    "\n",
    "Logistic regression is a fundamental machine learning algorithm used for binary classification problems. Unlike linear regression that predicts continuous values, logistic regression predicts the probability that an instance belongs to a particular category (0 or 1).\n",
    "\n",
    "### Key Concepts:\n",
    "- **Sigmoid Function**: Maps any real number to a value between 0 and 1\n",
    "- **Linear Combination**: z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
    "- **Probability**: P(y=1|x) = σ(z) = 1/(1 + e^(-z))\n",
    "- **Binary Cross-Entropy Loss**: Measures how far predictions are from actual labels\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand the mathematical foundation of logistic regression\n",
    "2. Implement logistic regression manually using PyTorch tensors\n",
    "3. Compare with PyTorch's built-in nn.Module implementation\n",
    "4. Visualize decision boundaries and training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Binary Logistic Regression with PyTorch ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We'll create a synthetic medical dataset for binary classification. The data preprocessing steps are crucial:\n",
    "\n",
    "1. **Standardization**: Centers data around 0 with unit variance\n",
    "2. **Train-Test Split**: Prevents overfitting by evaluating on unseen data\n",
    "3. **Tensor Conversion**: PyTorch requires data in tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generating synthetic medical data...\n",
      "Training samples: 800\n",
      "Test samples: 200\n",
      "Features: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Generating synthetic medical data...\")\n",
    "# Simulate medical features (e.g., age, blood pressure, BMI, etc.)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,  # 2 features for easy visualization\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Standardize features (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "print(f\"Training samples: {X_train_tensor.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_tensor.shape[0]}\")\n",
    "print(f\"Features: {X_train_tensor.shape[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Implementation\n",
    "\n",
    "Let's implement logistic regression from scratch to understand the underlying mathematics:\n",
    "\n",
    "### Mathematical Foundation:\n",
    "\n",
    "1. **Linear Combination**: z = Xw + b\n",
    "2. **Sigmoid Function**: σ(z) = 1/(1 + e^(-z))\n",
    "3. **Binary Cross-Entropy Loss**: L = -[y·log(ŷ) + (1-y)·log(1-ŷ)]\n",
    "4. **Gradient Descent**: w = w - α·∇L/∇w\n",
    "\n",
    "### Why Sigmoid?\n",
    "- Maps any real number to (0,1) range\n",
    "- Smooth, differentiable function\n",
    "- S-shaped curve naturally models probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionManual:\n",
    "    def __init__(self, input_dim):\n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = torch.randn(input_dim, 1, requires_grad=True)\n",
    "        self.bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clamp to prevent overflow\n",
    "        z = torch.clamp(z, -500, 500)\n",
    "        return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Linear combination: z = X @ w + b\n",
    "        z = torch.matmul(x, self.weights) + self.bias\n",
    "        # Apply sigmoid\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        # Prevent log(0) by adding small epsilon\n",
    "        eps = 1e-15\n",
    "        y_pred = torch.clamp(y_pred, eps, 1 - eps)\n",
    "\n",
    "        loss = -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        return torch.mean(loss)\n",
    "\n",
    "    def train_step(self, x, y, learning_rate=0.01):\n",
    "        \"\"\"One training step\"\"\"\n",
    "        # Forward pass\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        with torch.no_grad():\n",
    "            self.weights -= learning_rate * self.weights.grad\n",
    "            self.bias -= learning_rate * self.bias.grad\n",
    "\n",
    "            # Zero gradients for next iteration\n",
    "            self.weights.grad.zero_()\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Manual Model\n",
    "\n",
    "Now let's train our manually implemented logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training manual implementation...\n",
      "Epoch 0, Loss: 0.6636\n",
      "Epoch 200, Loss: 0.2686\n",
      "Epoch 400, Loss: 0.2467\n",
      "Epoch 600, Loss: 0.2398\n",
      "Epoch 800, Loss: 0.2368\n",
      "Manual model accuracy: 0.9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training manual implementation...\")\n",
    "manual_model = LogisticRegressionManual(input_dim=2)\n",
    "manual_losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = manual_model.train_step(X_train_tensor, y_train_tensor, learning_rate=0.1)\n",
    "    manual_losses.append(loss)\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test manual model\n",
    "with torch.no_grad():\n",
    "    manual_predictions = manual_model.forward(X_test_tensor)\n",
    "    manual_pred_classes = (manual_predictions > 0.5).float()\n",
    "    manual_accuracy = (manual_pred_classes == y_test_tensor).float().mean()\n",
    "    print(f\"Manual model accuracy: {manual_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch nn.Module Implementation\n",
    "\n",
    "Now let's implement the same model using PyTorch's built-in components:\n",
    "\n",
    "### Advantages of nn.Module:\n",
    "- **Automatic gradient computation**: No need to manually implement backpropagation\n",
    "- **Built-in optimizers**: SGD, Adam, RMSprop, etc.\n",
    "- **GPU support**: Easy transfer to GPU with `.cuda()`\n",
    "- **Model management**: Save/load, parameter counting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionPyTorch, self).__init__()\n",
    "        # Linear layer automatically initializes weights and bias\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear transformation followed by sigmoid\n",
    "        z = self.linear(x)\n",
    "        return self.sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the PyTorch Model\n",
    "\n",
    "Notice how much cleaner the training loop is with PyTorch's built-in components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "pytorch_model = LogisticRegressionPyTorch(input_dim=2)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(pytorch_model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training PyTorch model...\")\n",
    "pytorch_losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs = pytorch_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update parameters\n",
    "\n",
    "    pytorch_losses.append(loss.item())\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test PyTorch model\n",
    "pytorch_model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    pytorch_predictions = pytorch_model(X_test_tensor)\n",
    "    pytorch_pred_classes = (pytorch_predictions > 0.5).float()\n",
    "    pytorch_accuracy = (pytorch_pred_classes == y_test_tensor).float().mean()\n",
    "    print(f\"PyTorch model accuracy: {pytorch_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison\n",
    "\n",
    "Let's compare both implementations to verify they produce similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Comparison\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Manual Implementation Accuracy: {manual_accuracy:.4f}\")\n",
    "print(f\"PyTorch Implementation Accuracy: {pytorch_accuracy:.4f}\")\n",
    "\n",
    "# Compare final parameters\n",
    "print(\"\\nFinal Parameters:\")\n",
    "print(\"Manual model:\")\n",
    "print(f\"  Weights: {manual_model.weights.detach().numpy().flatten()}\")\n",
    "print(f\"  Bias: {manual_model.bias.detach().numpy().flatten()}\")\n",
    "\n",
    "print(\"PyTorch model:\")\n",
    "with torch.no_grad():\n",
    "    weights = pytorch_model.linear.weight.data.numpy().flatten()\n",
    "    bias = pytorch_model.linear.bias.data.numpy().flatten()\n",
    "    print(f\"  Weights: {weights}\")\n",
    "    print(f\"  Bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Let's visualize the training progress and decision boundary to better understand how our models learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Plot training losses\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(manual_losses, label=\"Manual Implementation\", alpha=0.7)\n",
    "plt.plot(pytorch_losses, label=\"PyTorch Implementation\", alpha=0.7)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "# Create mesh for decision boundary\n",
    "h = 0.1\n",
    "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "with torch.no_grad():\n",
    "    Z = pytorch_model(mesh_points).numpy()\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap=\"RdYlBu\")\n",
    "plt.colorbar(label=\"Probability\")\n",
    "scatter = plt.scatter(\n",
    "    X_scaled[:, 0], X_scaled[:, 1], c=y, cmap=\"RdYlBu\", edgecolors=\"black\"\n",
    ")\n",
    "plt.xlabel(\"Feature 1 (standardized)\")\n",
    "plt.ylabel(\"Feature 2 (standardized)\")\n",
    "plt.title(\"Decision Boundary\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Mathematical Foundation**: \n",
    "   - Logistic regression uses the sigmoid function to map linear combinations to probabilities\n",
    "   - Binary cross-entropy loss measures prediction quality\n",
    "   - Gradient descent optimizes parameters iteratively\n",
    "\n",
    "2. **Implementation Comparison**:\n",
    "   - Manual implementation helps understand the underlying mathematics\n",
    "   - PyTorch implementation is cleaner and more maintainable\n",
    "   - Both approaches achieve similar performance\n",
    "\n",
    "3. **PyTorch Advantages**:\n",
    "   - Automatic gradient computation\n",
    "   - Built-in optimizers and loss functions\n",
    "   - Easy model management and deployment\n",
    "   - GPU acceleration support\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Always standardize input features\n",
    "   - Use train/validation/test splits\n",
    "   - Monitor training loss to detect overfitting\n",
    "   - Visualize results to validate model behavior\n",
    "\n",
    "### Next Steps:\n",
    "- Try different optimizers (Adam, RMSprop)\n",
    "- Experiment with regularization techniques\n",
    "- Apply to real-world medical datasets\n",
    "- Explore multi-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ Implemented logistic regression manually  \n",
    "✓ Implemented using PyTorch nn.Module  \n",
    "✓ Both models achieve similar performance  \n",
    "✓ PyTorch provides cleaner, more maintainable code  \n",
    "✓ PyTorch handles gradient computation automatically  \n",
    "\n",
    "This notebook demonstrates the power of PyTorch for implementing machine learning algorithms while maintaining a deep understanding of the underlying mathematics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
